{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971e1c20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tradingday'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa8971ec37bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mredis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtradingday\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtradingday\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingDays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tradingday'"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from typing import List\n",
    "import redis\n",
    "from tradingday.tradingday import TradingDays\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=os.environ.get(\"LOGLEVEL\", \"INFO\"),\n",
    "    format=\"%(levelname)-3s %(name)s: %(message)s\",\n",
    ")\n",
    "\n",
    "\n",
    "class PubSubServiceBase(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        host=\"localhost\",\n",
    "        port=6379,\n",
    "        db=0,\n",
    "        sub_streams: List[str] = None,\n",
    "        pub_stream_by_default: str = None,\n",
    "        group: str = \"cg\",\n",
    "    ):\n",
    "        logger.info(\"\\n[{}] try init redis. \".format(self.__class__.__name__))\n",
    "        self.db_ = redis.Redis(host=host, port=port, db=db)\n",
    "\n",
    "        # ensure sub_streams exitst\n",
    "        for stream_obj in sub_streams:\n",
    "            if not self.db_.exists(stream_obj):\n",
    "                msg_id = self.db_.xadd(stream_obj, {\"\": \"\"}, id=b\"0-1\")\n",
    "                self.db_.xdel(stream_obj, msg_id)\n",
    "\n",
    "            # create\n",
    "            try:\n",
    "                self.db_.xgroup_create(\n",
    "                    name=stream_obj, groupname=group, id=\"0-0\", mkstream=False\n",
    "                )\n",
    "            except redis.exceptions.ResponseError as exc:\n",
    "                if not exc.args[0].startswith(\"BUSYGROUP\"):\n",
    "                    raise\n",
    "\n",
    "        logger.info(\n",
    "            \"\\n[{}] init redis ok. group is {}\\n[{}] sub stream(s) are {}\\n[{}] pub stream(s) are [{}]\".format(\n",
    "                self.__class__.__name__,\n",
    "                group,\n",
    "                self.__class__.__name__,\n",
    "                sub_streams,\n",
    "                self.__class__.__name__,\n",
    "                pub_stream_by_default,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.group_ = group\n",
    "        self.consumer_name_ = group + \".tosaka\"\n",
    "        # self.cg_.set_id() 默认行为F是从最后一次deliver的地方开始读，\n",
    "        # 这也是我们想要的，只要把每次pending的拿来消费就可以保证消息不丢失了\n",
    "\n",
    "        self.pub_stream_by_default_ = pub_stream_by_default\n",
    "        self.sub_streams_ = sub_streams\n",
    "\n",
    "        self.stop_flag_ = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def consume_message(self, stream, msg_id, content):\n",
    "        \"\"\"\n",
    "        定义如何消费一个监听到的消息(来自于 self.sub_streams), 消费顺利的话需要 ack 这个消息\n",
    "        默认是打印这个消息, 子类需要自定义如何消费\n",
    "\n",
    "        stream : source of message\n",
    "        msg_id : a message id(at least for ack)\n",
    "        content: content of the message\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def ack_message(self, stream, msg_id):\n",
    "        \"\"\"\n",
    "        处理完的消息都应该被及时 ack, 否则该 service 会认为该条消息没有消费成功，在 service 下一次\n",
    "        启动时, 可能会检查 pending 列表，再次消费该条消息\n",
    "\n",
    "        stream : source of message\n",
    "        msg_id : a message id(used to ack)\n",
    "\n",
    "        \"\"\"\n",
    "        self.db_.xack(stream, self.group_, msg_id)\n",
    "\n",
    "    def push_message(self, msg: dict, stream: str = None):\n",
    "        self.db_.xadd(\n",
    "            stream or self.pub_stream_by_default_,\n",
    "            msg,\n",
    "            id=\"*\",\n",
    "            maxlen=None,\n",
    "            approximate=True,\n",
    "        )\n",
    "\n",
    "    def resolve_pending(self):\n",
    "        # Check if has unack message\n",
    "        # for stream_obj in self.sub_streams_:\n",
    "        for stream_obj in self.sub_streams_:\n",
    "            pending_list = self.db_.xpending_range(\n",
    "                stream_obj,\n",
    "                self.group_,\n",
    "                min=\"-\",\n",
    "                max=\"+\",\n",
    "                count=10000000,\n",
    "                consumername=self.consumer_name_,\n",
    "            )\n",
    "\n",
    "            logger.info(\n",
    "                \"\\n[{}] pending list <{}> size is {}\".format(\n",
    "                    self.__class__.__name__, stream_obj, len(pending_list)\n",
    "                )\n",
    "            )\n",
    "            for pending_obj in pending_list:\n",
    "                msg_id = pending_obj[\"message_id\"]\n",
    "                msg_list = self.db_.xrange(\n",
    "                    name=stream_obj, min=msg_id, max=msg_id, count=1000000\n",
    "                )\n",
    "                msg_id, content = msg_list[0]\n",
    "                msg_id = str(msg_id, \"utf-8\")\n",
    "                content = {\n",
    "                    str(key, \"utf-8\"): str(value, \"utf-8\")\n",
    "                    for key, value in content.items()\n",
    "                }\n",
    "                self.consume_message(stream_obj, msg_id, content)\n",
    "\n",
    "    def try_read(self, block_time=20):\n",
    "        # stream_items is a list of [stream_name, message_list]\n",
    "        stream_items = self.db_.xreadgroup(\n",
    "            self.group_,\n",
    "            self.consumer_name_,\n",
    "            {i: \">\" for i in self.sub_streams_},\n",
    "            block=block_time,\n",
    "            count=100000000,\n",
    "        )\n",
    "        if len(stream_items) == 0:\n",
    "            return\n",
    "        # item is a list of stream_name and message_list\n",
    "        # item has only two elements\n",
    "        for item in stream_items:\n",
    "            stream_name = item[0]\n",
    "            msg_list = item[1]\n",
    "            for msg in msg_list:\n",
    "                msg_id, content = msg\n",
    "                msg_id = str(msg_id, \"utf-8\")\n",
    "                content = {\n",
    "                    str(key, \"utf-8\"): str(value, \"utf-8\")\n",
    "                    for key, value in content.items()\n",
    "                }\n",
    "                self.consume_message(stream_name, msg_id, content)\n",
    "\n",
    "    def listen(self, resolve_pending: bool = True):\n",
    "        if resolve_pending:\n",
    "            self.resolve_pending()\n",
    "\n",
    "        while not self.stop_flag_:\n",
    "            try:\n",
    "                self.try_read(block_time=100)\n",
    "            except redis.exceptions.ConnectionError as conn_error:\n",
    "                logger.error(\n",
    "                    f\"[{self.__class__.__name__}] Redis ConnectionError {conn_error}.\"\n",
    "                )\n",
    "                self.reconnect()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_flag_ = True\n",
    "\n",
    "    def reconnect(self):\n",
    "        stop_retry_flag = False\n",
    "        while not stop_retry_flag:\n",
    "            try:\n",
    "                _ = self.db_.exists(self.sub_streams_[0])\n",
    "                stop_retry_flag = True\n",
    "                logger.info(f\"[{self.__class__.__name__}] Redis reconnect success.\")\n",
    "            except Exception as e:\n",
    "                logger.error(\n",
    "                    f\"[{self.__class__.__name__}] Redis reconnect error {e} Retry in 5s.\"\n",
    "                )\n",
    "                time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "placed-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fuker(PubSubServiceBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        host=\"localhost\",\n",
    "        port=6379,\n",
    "        db=1,\n",
    "        sub_stream: str = \"\",\n",
    "        pub_stream: str = \"\",\n",
    "        group: str = \"Gourp_Demo\",\n",
    "        table_map={\n",
    "            \"JsyMinBarArcImporter\": \"jsy.MinBar\",\n",
    "        },\n",
    "        root: str = \"/test_root\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            db=db,\n",
    "            sub_streams = sub_stream,\n",
    "            pub_stream_by_default=pub_stream,\n",
    "            group=group,\n",
    "        )\n",
    "\n",
    "        self.table_map = table_map\n",
    "        self.root = root\n",
    "\n",
    "    def consume_message(self, stream, msg_id, content):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "moved-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO __main__: \n",
      "[Fuker] try init redis. \n",
      "INFO __main__: \n",
      "[Fuker] init redis ok. group is GroupJupyter\n",
      "[Fuker] sub stream(s) are FilesOfAccounting\n",
      "[Fuker] pub stream(s) are [FilesOfAccounting]\n"
     ]
    }
   ],
   "source": [
    "god = Fuker(host = \"bishop\", port = 6379, db = 6, sub_stream = \"FilesOfAccounting\", pub_stream = \"FilesOfAccounting\", group = \"GroupJupyter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "physical-arlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/lib/wonder/warehouse/mail/kunpeng1\n",
      "{'date': '20220614', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220614/qifei'}\n",
      "{'date': '20220523', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220523/qifei'}\n",
      "{'date': '20220525', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220525/qifei'}\n",
      "{'date': '20220527', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220527/qifei'}\n",
      "{'date': '20220621', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220621/qifei'}\n",
      "{'date': '20220630', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220630/qifei'}\n",
      "{'date': '20220715', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220715/qifei'}\n",
      "{'date': '20220622', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220622/qifei'}\n",
      "{'date': '20220705', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220705/qifei'}\n",
      "{'date': '20220708', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220708/qifei'}\n",
      "{'date': '20220526', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220526/qifei'}\n",
      "{'date': '20220615', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220615/qifei'}\n",
      "{'date': '20220712', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220712/qifei'}\n",
      "{'date': '20220714', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220714/qifei'}\n",
      "{'date': '20220628', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220628/qifei'}\n",
      "{'date': '20220524', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220524/qifei'}\n",
      "{'date': '20220607', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220607/qifei'}\n",
      "{'date': '20220711', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220711/qifei'}\n",
      "{'date': '20220613', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220613/qifei'}\n",
      "{'date': '20220606', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220606/qifei'}\n",
      "{'date': '20220713', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220713/qifei'}\n",
      "{'date': '20220718', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220718/qifei'}\n",
      "{'date': '20220620', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220620/qifei'}\n",
      "{'date': '20220707', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220707/qifei'}\n",
      "{'date': '20220719', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220719/qifei'}\n",
      "{'date': '20220616', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220616/qifei'}\n",
      "{'date': '20220530', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220530/qifei'}\n",
      "{'date': '20220602', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220602/qifei'}\n",
      "{'date': '20220629', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220629/qifei'}\n",
      "{'date': '20220704', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220704/qifei'}\n",
      "{'date': '20220706', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220706/qifei'}\n",
      "{'date': '20220610', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220610/qifei'}\n",
      "{'date': '20220608', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220608/qifei'}\n",
      "{'date': '20220601', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220601/qifei'}\n",
      "{'date': '20220609', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220609/qifei'}\n",
      "{'date': '20220623', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220623/qifei'}\n",
      "{'date': '20220701', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220701/qifei'}\n",
      "{'date': '20220531', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220531/qifei'}\n",
      "{'date': '20220617', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220617/qifei'}\n",
      "{'date': '20220627', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220627/qifei'}\n",
      "{'date': '20220624', 'book_id': 'lh_kunpeng1', 'file_path': '/var/lib/wonder/warehouse/mail/kunpeng1/20220624/qifei'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "books = [\"kunpeng1\"]\n",
    "ids = {\"dingli1\":\"zz_dingli1\", \"jinglun1\":\"zx_jinglun1\", \"jinglun2\":\"zx_jinglun2\", \"jinglun3\":\"zx_jinglun3\", \"kunpeng1\":\"lh_kunpeng1\"}\n",
    "for book in books:\n",
    "    root_path = Path(f\"/var/lib/wonder/warehouse/mail/{book}\")\n",
    "    print(root_path)\n",
    "    for child in root_path.iterdir():\n",
    "        message = {\"date\":f\"{child.name}\",\"book_id\":ids[book],\"file_path\":(root_path / child / \"qifei\").as_posix()}\n",
    "        print(message)\n",
    "        god.push_message(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
